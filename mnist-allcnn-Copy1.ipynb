{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataget import data\n",
    "import tfinterface as ti\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cytoolz as cz\n",
    "from dicto import dicto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataget\n",
    "To download and import the data we will use dataget here but you can import the mnist however you want, there are various ways to import the mnist like using keras sample datasets. The first time you run this code `dataget` will download, extract, and transform the `MNIST` to jpg images; any subsequent calls use the cached data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'class_id', u'filename'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = data(\"mnist\").get()\n",
    "\n",
    "dataset.training_set.df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.get` method returns a `Dataset` object which has a `training_set` and `test_set` `SubSet` properties, these subsets include varios methods for loading the images, getting random batches, etc. However, each subset by default include a `.df` property which the columns filename (full path to the actual image) and class_id (number show in the image), although you sometimes wan't a generator of (image, label) batches, we will make TensorFlow do the IO for us so just having the filenames is perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator API, input_fn & Dataset API\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def input_fn(df, img_shape = [28, 28], embeddings_shape = [1, 1, 100], epochs = 10, batch_size = 64, buffer_size = 500):\n",
    "    def parse(filename, class_id):\n",
    "\n",
    "        img = tf.read_file(filename)\n",
    "        img = tf.image.decode_jpeg(img, channels = 1)\n",
    "        img = tf.image.resize_images(img, img_shape)\n",
    "\n",
    "        return img, class_id\n",
    "\n",
    "    # ds\n",
    "    ds = tf.data.Dataset.from_tensor_slices((df.filename.as_matrix(), df.class_id.as_matrix()))\n",
    "    ds = ds.map(parse)\n",
    "    ds = ds.shuffle(buffer_size = buffer_size)\n",
    "    ds = ds.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\n",
    "    ds = ds.repeat(epochs)\n",
    "\n",
    "    # get iterator\n",
    "    iterator = ds.make_one_shot_iterator()\n",
    "    image, class_id = iterator.get_next()\n",
    "\n",
    "    return dict(image = image, class_id = class_id), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_id': <tf.Tensor 'IteratorGetNext:1' shape=(20,) dtype=int64>, 'image': <tf.Tensor 'IteratorGetNext:0' shape=(20, 28, 28, 1) dtype=float32>}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Fetch argument <tf.Tensor 'IteratorGetNext:0' shape=(20, 28, 28, 1) dtype=float32> cannot be interpreted as a Tensor. (Tensor Tensor(\"IteratorGetNext:0\", shape=(20, 28, 28, 1), dtype=float32) is not an element of this graph.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-dcd7adac80dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cristian/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cristian/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1105\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cristian/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \"\"\"\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cristian/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cristian/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \"\"\"\n\u001b[1;32m    340\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cristian/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;31m# Did not find anything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' %\n",
      "\u001b[0;32m/home/cristian/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    276\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n\u001b[0;32m--> 278\u001b[0;31m                          'Tensor. (%s)' % (fetch, str(e)))\n\u001b[0m\u001b[1;32m    279\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[0;31mValueError\u001b[0m: Fetch argument <tf.Tensor 'IteratorGetNext:0' shape=(20, 28, 28, 1) dtype=float32> cannot be interpreted as a Tensor. (Tensor Tensor(\"IteratorGetNext:0\", shape=(20, 28, 28, 1), dtype=float32) is not an element of this graph.)"
     ]
    }
   ],
   "source": [
    "features, _ = input_fn(dataset.training_set.df, batch_size=20)\n",
    "\n",
    "image = features[\"image\"]\n",
    "class_id = features[\"class_id\"]\n",
    "\n",
    "print(features)\n",
    "    \n",
    "images, class_ids = sess.run([image, class_id])\n",
    "\n",
    "for tup in cz.partition(5, zip(images, class_ids)):\n",
    "\n",
    "    fig, axes = plt.subplots(1, 5, subplot_kw=dict())\n",
    "\n",
    "    for i, (img, class_id) in enumerate(tup):\n",
    "\n",
    "\n",
    "        img = np.squeeze(img).astype(np.uint8)\n",
    "\n",
    "        axes[i].imshow(img, cmap=\"gray\")\n",
    "        axes[i].title.set_text(class_id)\n",
    "        axes[i].get_yaxis().set_visible(False)\n",
    "        axes[i].get_xaxis().set_visible(False)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllCNNN(snt.AbstractModule):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        kwargs[\"name\"] = kwargs.get(\"name\", \"AllCNN\")\n",
    "        super(AllCNNN, self).__init__(*args, **kwargs)\n",
    "    \n",
    "    def _build(self, inputs):\n",
    "        \n",
    "        print(\"##########################\")\n",
    "        print(\"## AllCNN\")\n",
    "        print(\"##########################\")\n",
    "        \n",
    "        net = inputs[\"image\"]; print(net)\n",
    "        training = inputs[\"mode\"] == tf.estimator.ModeKeys.TRAIN\n",
    "        \n",
    "        net = tf.layers.conv2d(net, 16, [4, 4], strides = 2, activation = tf.nn.relu, \n",
    "                                          padding = \"same\"); print(net)\n",
    "        \n",
    "        \n",
    "        net = tf.layers.conv2d(net, 32, [4, 4], strides = 2, activation = tf.nn.relu, \n",
    "                                          padding = \"same\"); print(net)\n",
    "        \n",
    "        \n",
    "        net = tf.layers.conv2d(net, 64, [3, 3], strides = 1, activation = tf.nn.relu, \n",
    "                                          padding = \"valid\"); print(net)\n",
    "        \n",
    "        \n",
    "        net = tf.layers.conv2d(net, 10, [3, 3], strides = 1, activation = tf.nn.relu,\n",
    "                                          padding = \"valid\"); print(net)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # global average pooling\n",
    "        logits = net = tf.reduce_mean(net, axis = [1, 2]); print(net)\n",
    "        \n",
    "        # predictions\n",
    "        predictions = net = tf.nn.softmax(logits); print(net)\n",
    "    \n",
    "        print(\"\")\n",
    "        \n",
    "        return logits, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################\n",
      "## AllCNN\n",
      "##########################\n",
      "Tensor(\"input_layer_2:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"AllCNN_1/conv2d/Relu:0\", shape=(?, 14, 14, 16), dtype=float32)\n",
      "Tensor(\"AllCNN_1/conv2d_2/Relu:0\", shape=(?, 7, 7, 32), dtype=float32)\n",
      "Tensor(\"AllCNN_1/conv2d_3/Relu:0\", shape=(?, 5, 5, 64), dtype=float32)\n",
      "Tensor(\"AllCNN_1/conv2d_4/Relu:0\", shape=(?, 3, 3, 10), dtype=float32)\n",
      "Tensor(\"AllCNN_1/Mean:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"AllCNN_1/Softmax:0\", shape=(?, 10), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = dict(\n",
    "    image = tf.layers.Input(shape=(28, 28, 1)),\n",
    "    mode = tf.estimator.ModeKeys.TRAIN,\n",
    ")\n",
    "\n",
    "all_cnn = AllCNNN()\n",
    "logits, predictions = all_cnn(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    \n",
    "    inputs = features\n",
    "    inputs[\"mode\"] = mode\n",
    "    \n",
    "    # create networks\n",
    "    all_cnn = AllCNNN()\n",
    "    \n",
    "    # predictions\n",
    "    logits, predictions = all_cnn(inputs)\n",
    "    \n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode = mode,\n",
    "            predictions = dict(\n",
    "                classes = predictions,\n",
    "                image = inputs[\"image\"],\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # loss\n",
    "    labels = inputs[\"class_id\"]\n",
    "    onehot_labels = tf.one_hot(labels, 10)\n",
    "    \n",
    "    loss = tf.losses.softmax_cross_entropy(logits = logits, onehot_labels = onehot_labels)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode = mode,\n",
    "            predictions = predictions,\n",
    "            loss = loss,\n",
    "            eval_metric_ops = dict(\n",
    "                accuracy = tf.metrics.accuracy(\n",
    "                    labels = labels,\n",
    "                    predictions = tf.argmax(predictions, axis = 1)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    #update\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        update = tf.train.AdamOptimizer(params[\"learning_rate\"]).minimize(loss, global_step = tf.train.get_global_step())\n",
    "        \n",
    "    \n",
    "    accuracy = tf.contrib.metrics.accuracy(\n",
    "        labels = labels,\n",
    "        predictions = tf.argmax(predictions, axis = 1)\n",
    "    )\n",
    "        \n",
    "    # metrics\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    \n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode = mode,\n",
    "        predictions = predictions,\n",
    "        loss = loss,\n",
    "        train_op = update,\n",
    "        training_hooks = [\n",
    "            tf.train.LoggingTensorHook(\n",
    "                dict(\n",
    "                    loss = loss, \n",
    "                    accuracy = accuracy\n",
    "                ),  \n",
    "                every_n_iter = 50\n",
    "            )\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f976e4fbc50>, '_save_checkpoints_steps': 50, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'models/all_cnn3', '_save_summary_steps': 50}\n"
     ]
    }
   ],
   "source": [
    "params = dicto.load_(\"parameters.yml\")\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "    model_fn = model_fn,\n",
    "    model_dir = params.model_dir,\n",
    "    params = params,\n",
    "    config = tf.estimator.RunConfig(save_checkpoints_steps = params.checkpoint_steps, save_summary_steps = params.summary_steps)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################\n",
      "## AllCNN\n",
      "##########################\n",
      "Tensor(\"IteratorGetNext:0\", shape=(64, 28, 28, 1), dtype=float32, device=/device:CPU:0)\n",
      "Tensor(\"AllCNN/conv2d/Relu:0\", shape=(64, 14, 14, 16), dtype=float32)\n",
      "Tensor(\"AllCNN/conv2d_2/Relu:0\", shape=(64, 7, 7, 32), dtype=float32)\n",
      "Tensor(\"AllCNN/conv2d_3/Relu:0\", shape=(64, 5, 5, 64), dtype=float32)\n",
      "Tensor(\"AllCNN/conv2d_4/Relu:0\", shape=(64, 3, 3, 10), dtype=float32)\n",
      "Tensor(\"AllCNN/Mean:0\", shape=(64, 10), dtype=float32)\n",
      "Tensor(\"AllCNN/Softmax:0\", shape=(64, 10), dtype=float32)\n",
      "\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into models/all_cnn3/model.ckpt.\n",
      "INFO:tensorflow:loss = 5.90478, step = 1\n",
      "INFO:tensorflow:loss = 5.90478, accuracy = 0.109375\n",
      "INFO:tensorflow:Saving checkpoints for 51 into models/all_cnn3/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.861252, accuracy = 0.78125 (42.767 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 101 into models/all_cnn3/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.16378\n",
      "INFO:tensorflow:loss = 0.715513, step = 101 (85.928 sec)\n",
      "INFO:tensorflow:loss = 0.715513, accuracy = 0.796875 (43.161 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 151 into models/all_cnn3/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.348701, accuracy = 0.890625 (42.148 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 201 into models/all_cnn3/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.18499\n",
      "INFO:tensorflow:loss = 0.62276, step = 201 (84.388 sec)\n",
      "INFO:tensorflow:loss = 0.62276, accuracy = 0.8125 (42.241 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 251 into models/all_cnn3/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.34135, accuracy = 0.890625 (43.186 sec)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7f3795772842>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m estimator.train(\n\u001b[0;32m----> 4\u001b[0;31m     input_fn = lambda: input_fn(\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cristian/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cristian/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m           \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cristian/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    519\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cristian/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    890\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    893\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m/home/cristian/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cristian/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cristian/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cristian/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cristian/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cristian/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cristian/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cristian/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = dicto.load_(\"parameters.yml\")\n",
    "\n",
    "estimator.train(\n",
    "    input_fn = lambda: input_fn(\n",
    "        dataset.training_set.df, \n",
    "        batch_size = params.batch_size, \n",
    "        epochs = params.epochs,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################\n",
      "## AllCNN\n",
      "##########################\n",
      "Tensor(\"IteratorGetNext:0\", shape=(64, 28, 28, 1), dtype=float32, device=/device:CPU:0)\n",
      "Tensor(\"AllCNN/Conv2dBatchNorm/Elu:0\", shape=(64, 14, 14, 16), dtype=float32)\n",
      "Tensor(\"AllCNN/Conv2dBatchNorm_1/Elu:0\", shape=(64, 7, 7, 32), dtype=float32)\n",
      "Tensor(\"AllCNN/Conv2dBatchNorm_2/Elu:0\", shape=(64, 5, 5, 64), dtype=float32)\n",
      "Tensor(\"AllCNN/conv2d/Elu:0\", shape=(64, 3, 3, 10), dtype=float32)\n",
      "Tensor(\"AllCNN/conv2d/Elu:0\", shape=(64, 3, 3, 10), dtype=float32)\n",
      "\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-19-21:06:45\n",
      "INFO:tensorflow:Restoring parameters from models/all_cnn1/model.ckpt-5601\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-19-21:10:25\n",
      "INFO:tensorflow:Saving dict for global step 5601: accuracy = 0.990324, global_step = 5601, loss = 0.0323472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.99032396, 'global_step': 5601, 'loss': 0.032347161}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = dicto.load_(\"parameters.yml\")\n",
    "\n",
    "estimator.evaluate(\n",
    "    input_fn = lambda: input_fn(\n",
    "        dataset.test_set.df, \n",
    "        batch_size = params.batch_size, \n",
    "        epochs = 1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dicto' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4bc3260aaae7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdicto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parameters.yml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m iterator = estimator.predict(\n\u001b[1;32m      4\u001b[0m     input_fn = lambda: input_fn(\n\u001b[1;32m      5\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dicto' is not defined"
     ]
    }
   ],
   "source": [
    "params = dicto.load_(\"parameters.yml\")\n",
    "\n",
    "iterator = estimator.predict(\n",
    "    input_fn = lambda: input_fn(\n",
    "        dataset.test_set.df, \n",
    "        batch_size = 64, \n",
    "        epochs = 1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "##########################\n",
      "## AllCNN\n",
      "##########################\n",
      "Tensor(\"IteratorGetNext:0\", shape=(64, 28, 28, 1), dtype=float32, device=/device:CPU:0)\n",
      "Tensor(\"AllCNN/Conv2dBatchNorm/Elu:0\", shape=(64, 14, 14, 16), dtype=float32)\n",
      "Tensor(\"AllCNN/Conv2dBatchNorm_1/Elu:0\", shape=(64, 7, 7, 32), dtype=float32)\n",
      "Tensor(\"AllCNN/Conv2dBatchNorm_2/Elu:0\", shape=(64, 5, 5, 64), dtype=float32)\n",
      "Tensor(\"AllCNN/conv2d/Elu:0\", shape=(64, 3, 3, 10), dtype=float32)\n",
      "Tensor(\"AllCNN/Mean:0\", shape=(64, 10), dtype=float32)\n",
      "Tensor(\"AllCNN/Softmax:0\", shape=(64, 10), dtype=float32)\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from models/all_cnn1/model.ckpt-5601\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAAD7CAYAAABt9agKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQMKFQMKFQP8GBaTgkI7SQ1IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f02b1984250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = next(iterator)\n",
    "\n",
    "img = predictions[\"image\"]\n",
    "class_id = np.argmax(predictions[\"classes\"])\n",
    "\n",
    "ax = plt.axes()\n",
    "\n",
    "plt.imshow(img[:, :, 0], cmap=\"gray\")\n",
    "\n",
    "ax.title.set_text(class_id)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "ax.get_xaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################\n",
      "## AllCNN\n",
      "##########################\n",
      "Tensor(\"input_layer_1:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"AllCNN/conv2d/Relu:0\", shape=(?, 14, 14, 16), dtype=float32)\n",
      "Tensor(\"AllCNN/conv2d_2/Relu:0\", shape=(?, 7, 7, 32), dtype=float32)\n",
      "Tensor(\"AllCNN/conv2d_3/Relu:0\", shape=(?, 5, 5, 64), dtype=float32)\n",
      "Tensor(\"AllCNN/conv2d_4/Relu:0\", shape=(?, 3, 3, 10), dtype=float32)\n",
      "Tensor(\"AllCNN/Mean:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"AllCNN/Softmax:0\", shape=(?, 10), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = dicto.load_(\"parameters.yml\")\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    inputs = dict(\n",
    "        image = tf.layers.Input(shape=(28, 28, 1)),\n",
    "        mode = tf.estimator.ModeKeys.TRAIN,\n",
    "    )\n",
    "\n",
    "    all_cnn = AllCNNN()\n",
    "    logits, predictions = all_cnn(inputs)\n",
    "    \n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/all_cnn3/model.ckpt-251\n"
     ]
    }
   ],
   "source": [
    "params = dicto.load_(\"parameters.yml\")\n",
    "\n",
    "sess_config = tf.ConfigProto()\n",
    "sess_config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(graph=graph, config=sess_config)\n",
    "snapshot_fpath = tf.train.latest_checkpoint(params.model_dir)\n",
    "saver.restore(sess, snapshot_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 8 variables.\n",
      "Converted 8 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./frozen_models/all_cnn1.pb'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.framework import graph_io\n",
    "\n",
    "params = dicto.load_(\"parameters.yml\")\n",
    "\n",
    "graphdef_inf = tf.graph_util.remove_training_nodes(graph.as_graph_def())\n",
    "\n",
    "graphdef_frozen = tf.graph_util.convert_variables_to_constants(\n",
    "    sess, graphdef_inf, params.OUTPUT_NAMES)\n",
    "\n",
    "graph_io.write_graph(graphdef_frozen, './', params.FROZEN_FPATH, as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'input_layer_1',\n",
       " u'AllCNN/conv2d/kernel',\n",
       " u'AllCNN/conv2d/bias',\n",
       " u'AllCNN/conv2d/Conv2D',\n",
       " u'AllCNN/conv2d/BiasAdd',\n",
       " u'AllCNN/conv2d/Relu',\n",
       " u'AllCNN/conv2d_1/kernel',\n",
       " u'AllCNN/conv2d_1/bias',\n",
       " u'AllCNN/conv2d_2/Conv2D',\n",
       " u'AllCNN/conv2d_2/BiasAdd',\n",
       " u'AllCNN/conv2d_2/Relu',\n",
       " u'AllCNN/conv2d_2/kernel',\n",
       " u'AllCNN/conv2d_2/bias',\n",
       " u'AllCNN/conv2d_3/Conv2D',\n",
       " u'AllCNN/conv2d_3/BiasAdd',\n",
       " u'AllCNN/conv2d_3/Relu',\n",
       " u'AllCNN/conv2d_3/kernel',\n",
       " u'AllCNN/conv2d_3/bias',\n",
       " u'AllCNN/conv2d_4/Conv2D',\n",
       " u'AllCNN/conv2d_4/BiasAdd',\n",
       " u'AllCNN/conv2d_4/Relu',\n",
       " u'AllCNN/Mean/reduction_indices',\n",
       " u'AllCNN/Mean',\n",
       " u'AllCNN/Softmax']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.name for x in graphdef_frozen.node]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import uff\n",
    "from tensorrt.parsers import uffparser\n",
    "\n",
    "MAX_WORKSPACE = 1 << 20 # ADJUST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using output node AllCNN/Softmax\n",
      "Converting to UFF graph\n",
      "No. nodes: 24\n"
     ]
    }
   ],
   "source": [
    "params = dicto.load_(\"parameters.yml\")\n",
    "\n",
    "uff_model = uff.from_tensorflow_frozen_model(\n",
    "    params.FROZEN_FPATH, \n",
    "    params.OUTPUT_NAMES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dicto.load_(\"parameters.yml\")\n",
    "\n",
    "parser = uffparser.create_uff_parser()\n",
    "\n",
    "for input_ in params.INPUTS:\n",
    "    parser.register_input(input_.name, input_.size, 0)\n",
    "\n",
    "for output in params.OUTPUT_NAMES:\n",
    "    parser.register_output(output)\n",
    "\n",
    "G_LOGGER = trt.infer.ConsoleLogger(trt.infer.LogSeverity.INTERNAL_ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = trt.utils.uff_to_trt_engine(\n",
    "    G_LOGGER, uff_model, parser, params.MAX_BATCH_SIZE, MAX_WORKSPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trt.utils.write_engine_to_file(params.ENGINE_FPATH, engine.serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorrt.lite.engine.Engine"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = trt.lite.Engine(PLAN=params.ENGINE_FPATH)\n",
    "type(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "im = cv2.imread(dataset.test_set.df.filename.iloc[100])\n",
    "im = np.mean(im, axis=2, keepdims=True)\n",
    "\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICnCDyRF+IGkCD+QFOEHkvovtW09077aCcMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f022182e090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(im.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "out = engine.infer(im)\n",
    "\n",
    "print('Prediction: {}'.format(np.argmax(out[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################\n",
      "## AllCNN\n",
      "##########################\n",
      "Tensor(\"input_layer_1:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"AllCNN/conv2d/Relu:0\", shape=(?, 14, 14, 16), dtype=float32)\n",
      "Tensor(\"AllCNN/conv2d_2/Relu:0\", shape=(?, 7, 7, 32), dtype=float32)\n",
      "Tensor(\"AllCNN/conv2d_3/Relu:0\", shape=(?, 5, 5, 64), dtype=float32)\n",
      "Tensor(\"AllCNN/conv2d_4/Relu:0\", shape=(?, 3, 3, 10), dtype=float32)\n",
      "Tensor(\"AllCNN/Mean:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"AllCNN/Softmax:0\", shape=(?, 10), dtype=float32)\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from models/all_cnn3/model.ckpt-251\n",
      "INFO:tensorflow:Froze 8 variables.\n",
      "Converted 8 variables to const ops.\n",
      "Using output node AllCNN/Softmax\n",
      "Converting to UFF graph\n",
      "No. nodes: 24\n"
     ]
    }
   ],
   "source": [
    "params = dicto.load_(\"parameters.yml\")\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.Session(graph = graph) as sess:\n",
    "    \n",
    "    inputs = dict(\n",
    "        image = tf.layers.Input(shape=(28, 28, 1)),\n",
    "        mode = tf.estimator.ModeKeys.TRAIN,\n",
    "    )\n",
    "\n",
    "    all_cnn = AllCNNN()\n",
    "    logits, predictions = all_cnn(inputs)\n",
    "    \n",
    "    graph_def = graph.as_graph_def()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    snapshot_fpath = tf.train.latest_checkpoint(params.model_dir)\n",
    "    saver.restore(sess, snapshot_fpath)\n",
    "    \n",
    "    # freeze graph and remove nodes used for training \n",
    "    frozen_graph = tf.graph_util.convert_variables_to_constants(sess, graph_def, params.OUTPUT_NAMES)\n",
    "    frozen_graph = tf.graph_util.remove_training_nodes(frozen_graph)\n",
    "    # Create UFF model and dump it on disk \n",
    "    uff_model = uff.from_tensorflow(frozen_graph, params.OUTPUT_NAMES)\n",
    "    dump = open('all_cnn.uff', 'wb')\n",
    "    dump.write(uff_model)\n",
    "    dump.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/cristian/anaconda2/lib/python2.7/site-packages/tensorrt/utils/_utils.py\", line 214, in uff_to_trt_engine\n",
      "    assert(engine)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "UFF parsing failed on line 214 in statement assert(engine)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a8d024a907e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m                                      \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 1 sample at a time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                                      \u001b[0mmax_workspace_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 1 GB GPU memory workspace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                                      datatype=trt.infer.DataType.FLOAT) # that's very cool, you can set precision\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_execution_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cristian/anaconda2/lib/python2.7/site-packages/tensorrt/utils/_utils.pyc\u001b[0m in \u001b[0;36muff_to_trt_engine\u001b[0;34m(logger, stream, parser, max_batch_size, max_workspace_size, datatype, plugin_factory, calibrator)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UFF parsing failed on line {} in statement {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: UFF parsing failed on line 214 in statement assert(engine)"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# This is a helper function, provided by TensorRT devs, to run inference\n",
    "def infer(context, input_img, batch_size):\n",
    "    \n",
    "    # load engine\n",
    "    engine = context.get_engine()\n",
    "    assert(engine.get_nb_bindings() == 2)\n",
    "    \n",
    "    # create output array to receive data\n",
    "    dims = engine.get_binding_dimensions(1).to_DimsCHW()\n",
    "    elt_count = dims.C() * dims.H() * dims.W() * batch_size\n",
    "    \n",
    "    # convert input data to Float32\n",
    "    input_img = input_img.astype(np.float32)\n",
    "    \n",
    "    # Allocate pagelocked memory\n",
    "    output = cuda.pagelocked_empty(elt_count, dtype=np.float32)\n",
    "    \n",
    "    # alocate device memory\n",
    "    d_input = cuda.mem_alloc(batch_size * input_img.size * input_img.dtype.itemsize)\n",
    "    d_output = cuda.mem_alloc(batch_size * output.size * output.dtype.itemsize)\n",
    "    bindings = [int(d_input), int(d_output)]\n",
    "    stream = cuda.Stream()\n",
    "    \n",
    "    # transfer input data to device\n",
    "    cuda.memcpy_htod_async(d_input, input_img, stream)\n",
    "    \n",
    "    # execute model\n",
    "    context.enqueue(batch_size, bindings, stream.handle, None)\n",
    "    \n",
    "    # transfer predictions back\n",
    "    cuda.memcpy_dtoh_async(output, d_output, stream)\n",
    "    \n",
    "    # return predictions\n",
    "    return output\n",
    "\n",
    "# load model\n",
    "uff_model = open('all_cnn.uff', 'rb').read()\n",
    "\n",
    "# create model parser\n",
    "parser = uffparser.create_uff_parser()\n",
    "\n",
    "for input in params.INPUTS:\n",
    "    parser.register_input(input.name, input.size, 0)\n",
    "\n",
    "for output in params.OUTPUT_NAMES:\n",
    "    parser.register_output(output)\n",
    "    \n",
    "# create inference engine and context (aka session)\n",
    "trt_logger = trt.infer.ConsoleLogger(trt.infer.LogSeverity.ERROR)\n",
    "engine = trt.utils.uff_to_trt_engine(logger=trt_logger,\n",
    "                                     stream=uff_model,\n",
    "                                     parser=parser,\n",
    "                                     max_batch_size=2, # 1 sample at a time\n",
    "                                     max_workspace_size= 1 << 30, # 1 GB GPU memory workspace\n",
    "                                     datatype=trt.infer.DataType.FLOAT) # that's very cool, you can set precision\n",
    "context = engine.create_execution_context()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
